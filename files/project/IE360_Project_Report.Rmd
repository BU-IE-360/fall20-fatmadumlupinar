---
title: "IE360 Fall Project"
author: "Fatma Nur Dumlupınar-Ali Ozan Memetoğlu-Alican Yılmaz"
date: "15 02 2021"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    code_folding: hide
    theme: journal
  pdf_document:
    toc: yes
    toc_depth: '3'
subtitle: Group 2
---


# Introduction

The aim of this project is to forecast the next-day’s hourly consumption values for Turkish electricity market. Since hourly prediction is required, we are expected to provide 24 predictions corresponding to the hours of the next day. Since the actual values of the previous day will be added to the data every day, the model will be updated and the model will be run again. 

Hourly electricity consumption values is available since 2017 in the data provided by EPİAŞ together with hourly temperature values of 7 different locations in Turkey. 

```{r message=FALSE,warning=FALSE}
setwd("C:/Users/fatma/Desktop/IE360/project/rapor")
Consumption<-read.csv("bulk_consumption_with_temp.csv")
Consumption$Date<-as.Date(Consumption$Date)
plot(ts(Consumption[,3:10],freq=24))

decomposed = decompose(ts(Consumption$Consumption,freq=24))
plot(decomposed) 

```


At the beginning of the submission period, on February 28th, we had data up to February 27th. Every day, the previous day's data is added to our dataset and our best model is updated with new data. We made our last estimate on February 12th with the current data up to February 11th. 
For modelling, Exponential Smoothing, Auto Regressive Integrated Moving Average(ARIMA), Auto Regressive Integrated Moving Average with Exogeneous Input (ARIMAX) and Regression models are used for estimation. Then, the one which gives the best result is selected. For evaluation, mean absolute percentage error(MAPE) and weighted mean absolute percentage error(WMAPE) metrics are considered. 


```{r message=FALSE,warning=FALSE}
# install the required packages first
require(lubridate)
require(zoo)
require(mgcv)
library("readxl")
require(jsonlite)
require(httr)
require(data.table)
require(forecast)
require(ggplot2)
get_token <- function(username, password, url_site){
    
    post_body = list(username=username,password=password)
    post_url_string = paste0(url_site,'/token/')
    result = POST(post_url_string, body = post_body)

    # error handling (wrong credentials)
    if(result$status_code==400){
        print('Check your credentials')
        return(0)
    }
    else if (result$status_code==201){
        output = content(result)
        token = output$key
    }

    return(token)
}

get_data <- function(start_date='2020-03-20', token, url_site){
    
    post_body = list(start_date=start_date,username=username,password=password)
    post_url_string = paste0(url_site,'/dataset/')
    
    header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
    result = GET(post_url_string, header, body = post_body)
    output = content(result)
    data = data.table::rbindlist(output)
    data[,event_date:=as.Date(event_date)]
    data = data[order(event_date)]
    return(data)
}


send_submission <- function(predictions, token, url_site, submit_now=F){
    
    format_check=check_format(predictions)
    if(!format_check){
        return(FALSE)
    }
    
    post_string="list("
    for(i in 1:nrow(predictions)){
        if(i<nrow(predictions)){
            post_string=sprintf("%s%s,",post_string,predictions$forecast[i])
        } else {
            post_string=sprintf("%s%s)",post_string,predictions$forecast[i])
        }
    }
    
    submission = eval(parse(text=post_string))
    json_body = jsonlite::toJSON(submission, auto_unbox = TRUE)
    submission=list(submission=json_body)
    
    print(submission)
    # {"31515569":2.4,"32939029":2.4,"4066298":2.4,"6676673":2.4,"7061886":2.4,"85004":2.4} 

    if(!submit_now){
        print("You did not submit.")
        return(FALSE)      
    }
    

    header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
    post_url_string = paste0(url_site,'/submission/')
    result = POST(post_url_string, header, body=submission)
    
    if (result$status_code==201){
        print("Successfully submitted. Below you can see the details of your submission")
    } else {
        print("Could not submit. Please check the error message below, contact the assistant if needed.")
    }
    
    print(content(result))
    
}

check_format <- function(predictions){
    
    if(is.data.frame(predictions) | is.data.frame(predictions)){
        if('forecast' %in% names(predictions)){
            if(nrow(predictions)==24){
                if(all(is.numeric(predictions$forecast))){
                    print("Format OK")
                    return(TRUE)
                } else {
                    print("forecast information is not numeric")
                    return(FALSE)                
                }
            } else {
                print("Forecasts for 24 hours should be provided, current number of rows:")
                print(nrow(predictions))
                return(FALSE)     
            }
        } 
    } else {
        print("Wrong format. Please provide data.frame or data.table object")
        return(FALSE)
    }
    
}

# this part is main code
subm_url = 'http://46.101.124.77'

u_name = "Group2"
p_word = "OeYuMdVtumOiG5ga"
submit_now = FALSE

username = u_name
password = p_word

token = get_token(username=u_name, password=p_word, url=subm_url)
data = get_data(token=token,url=subm_url)

# this part is where you need to provide your forecasting function / or set of R codes
predictions=data.table(Date=rep(as.Date(Sys.time())+1,24),Hour=0:23)
# be sure if ordered
predictions=predictions[order(Date,Hour)]
# dummy forecast
predictions[,forecast:=1:24]

#send_submission(predictions, token, url=subm_url, submit_now=F)

```


# Literature

Forecasting "short-term electricity consumption" has been studied extensively from different aspects. ARIMA models, regression models and neural networks have been developed to forecast the electricity consumption. In the context of our project, we focused on the "short-term" forecasting studies, that is, the models used to predict hourly, daily or weekly consumption. In most of the the neural network and regression models temperature, humidity, calendar times data have been used widely. Behzat Ecem Tutu et al.(2018) developed a seasonal ARIMA model(SARIMA(1,1,0)(1,0,1)7) to estimate daily electricity consumption in Turkey for a day later. In our study, we used mainly two different approaches: Time Series Approach and Regression Approach. During the project preparation phase, we have extensively benefited from "Forecasting: Principles and Practice" by Rob J. Hyndman and lecture notes of the IE-360 course.


# Approach

Before starting the `model selection/ parameter selection` phase, first, data is analyzed to understand its characteristics better.After `exploratory data analysis` phase, the model selection phase starts. Finally, to be able to test the overall performance of the prediction, statistical error measures are used and forecast is done for the competition period. During the model selection phase, several approaches have been developed and tested. Mainly, forecasting with time series analysis(including exponential smoothing, ARIMA, ARIMAX) and forecasting with regression approaches have been used in the scope of this project.

For the time series approach, each hour is modeled separately with a for loop. And during the competition period, predictions were sent to the system via pre-prepared application programming interface. To train the model each day, similar to sliding window approach is used, that is, each day the model has been updated to cover all the days up until the 2 days before of prediction day.


## Reading and Cleaning Data 

Train data and prediction data is combined for the further model selection part. Before combining the two sets, necessary data manipulation has been conducted(i.e. Date conversion). To get the prediction data, `get_data()` function was already given:

```{r reading and merging two data sets, message=FALSE,warning=FALSE}
setwd("C:/Users/fatma/Desktop/IE360/project/rapor")

df_2<-get_data(start_date="2021-01-08",token, subm_url) #test data
df1 = fread('bulk_consumption_with_temp.csv', header=TRUE)
df1[,Date:=as.Date(Date,"%m/%d/%Y")]
combined_data<-rbind(df1, df_2,use.names=FALSE)
```


As for the data cleaning, `tsclean` function of the forecast package was used. This function "identify and replace outliers and missing values in a time series". The effect of `tsclean()` can be seen below, visually:



```{r reading and cleaning data, message=FALSE,warning=FALSE}
setwd("C:/Users/fatma/Desktop/IE360/project/rapor")

consumption = fread('RealTimeConsumption-01012017-08012021.csv', header=TRUE)
names(consumption)[names(consumption) == "Consumption (MWh)"] <- "cons"
consumption[,Date:=as.Date(Date,"%d.%m.%Y")]
consumption[,cons:=as.numeric(gsub(",", "", cons))]
daily_consumption=consumption[,list(mean_consumption=mean(cons,na.rm=T)),by=list(Date)]

#mean take
#is.ts(daily_consumption)
daily_consumption
cons_ts = ts(daily_consumption[, c('mean_consumption')])

daily_consumption$clean_cons = tsclean(cons_ts)

#cleaned data plot
ts_data<-ts(daily_consumption,start = c(2017, 1,1),frequency = 365)
autoplot(ts_data[,c("mean_consumption","clean_cons")]) +
  theme_classic()+
  labs( x="Date",y="Consumption(Mwh)",title=("Daily Mean Electricity Consumption(Outliers removed vs not removed)"))
```

## Exploratory Data Analysis

To understand better the data, first it is plotted with 7 days and 30 days moving averages included:

```{r ma plots, message=FALSE,warning=FALSE}
daily_consumption$cons_ma = ma(daily_consumption$clean_cons, order=7) # using the clean count with no outliers
daily_consumption$cons_ma30 = ma(daily_consumption$clean_cons, order=30)


ggplot() + 
  geom_line(data = daily_consumption, aes(x = Date, y = clean_cons, colour = "Counts")) +
  geom_line(data = daily_consumption, aes(x = Date, y = cons_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = daily_consumption, aes(x = Date, y = cons_ma30, colour = "Monthly Moving Average"))  +
  labs(y='Electricity Consumption',title="Moving Average Trend Plot")

```

Weekly moving average shows the overall trend slightly better.


From the correlation plot below, we can see that there is strong correlation between the temperature values. However, the correlation between the consumption and temperature is found to be low.


```{r correlation plot, message=FALSE, warning=FALSE}

consumption_data = fread('bulk_consumption_with_temp.csv', header=TRUE)

consumption_data[,Date:=as.Date(Date,"%m/%d/%Y")]


library(corrplot)
consumption_data_complete<-consumption_data[complete.cases(consumption_data), ]

M<-cor(consumption_data_complete[,-2:-1])

corrplot(M, method="number")



```


## Time Series Analysis Approach

Time series approach is proposed for this forecasting project mainly because we can assume that external factors are embedded in the previous observations. Thus, using past data to estimate future data might give us a good result to some extent, we hoped. Two main methods that is widely used are `ARIMA` and `ETS(exponential smoothing)`, which we will use in our model selection phase. Although
 the two have some commonalities, there are some differences such as ARIMA models do not have exponential smoothing counterparts. 
 
After exaploratory analysis and ACF/PACF plots results, we observed seasonality at `weekly level`. This result from our analysis  also makes sense in real life as, for example, we can assume this monday's electricity consumption to be closely related with last monday's electricity consumption.
 
```{r decompose, message=FALSE,warning=FALSE}
ts_data1 = ts(na.omit(daily_consumption$clean_cons), frequency=7) 
decomp = decompose(ts_data1)
plot(decomp) 

```

The effect of seasonal componentsare `-3924.26926    73.60447   982.71621  1209.56060  1295.51325   998.91780  -636.04307`, respectively. The random component seems stationary, and it can be tested statisticaly with `KPSS Unit Root Test`:

```{r stationarity test, message=FALSE,warning=FALSE}
require(urca)
unt_test=ur.kpss(decomp$random) 
summary(unt_test)

```

Test statistics is 0.0033, which is quite low. Comparing the value with critical value at alpha level of 0.01 (0.0033<0.347)
, we can say that our data is stationary. Thus, differencing is not required.


To decide on the model parameters for both of the time series approaches, we could use ACF/PACF plots and compare the AIC/BIC/AICc values of the model. Or `auto.arima()` function can be used. 

### Forecasting with ARIMA

As mentioned above, model fitting was done separately for each hour. First, empty vector of size `24` is created to be filled with predicted values for each hour. Then, the last date that is to be used in model fitting is determined.(This value has been updated each day manually during the competition period to include all the information up until the prediction day.). Inside the for loop, arima model corresponding to each hour in a day is fitted and two days ahead prediction is, then, stored in the empty vector for each hour:

```{r FORECASTING WITH ARIMA3, message=FALSE,warning=FALSE,eval=FALSE,class.source="fold-show"}
# empty vector for predictions of 2-days ahead 
desired_length <- 24 
empty_vec <- rep(NA, desired_length)

current_date="2021-01-05" #This date is to be changed every day during the competition to include all the information.

# Forecasting for each hour.
for (i in 0:23){
hour=i
combined_data<-rbind(df1, df_2,use.names=FALSE)
#Data for model fitting
combined_data<-combined_data[Date<=current_date]
#Hour filtering
combined_data=combined_data[combined_data$Hour==hour,c("Date","Consumption")]

#outlier removal
combined_data$clean_data=tsclean(combined_data$Consumption)

combined_data<-ts(combined_data$clean_data,freq=7)


nahead=2 # 2 days later prediction

fitted=auto.arima(combined_data,seasonal = T,trace=F)

#Forecasting
forecasted1=forecast(fitted,h=nahead)
forecasted1

empty_vec[i+1]<-forecasted1$mean[2]
}
```

Now that, empty vector is created, prediction data.frame can be constructed to send the forecast predictions to the system.

```{r sending predictions to the system, message=FALSE,warning=FALSE,eval=FALSE}

predictions=data.table(Date=rep(as.Date(Sys.time())+1,24),Hour=0:23)
predictions
# be sure if ordered
predictions=predictions[order(Date,Hour)]
# dummy forecast

predictions[,forecast:=empty_vec]
predictions

#send_submission(predictions, token, url=url_site, submit_now=F)
```


### Forecasting with ARIMA on Random Component of the Time Series Object

The model is set in a similar way as in the previous part. However, data is filtered by hour and a model is set on the random component for each hour using auto.arima().  Before modelling, time series objects are created with frequency equal to 7 because of the high autocorrelation value at lag 7. In other words, a consumption value at an hour is correlated with the value at the same hour in the previous week. Decomposition of the time series is made in additive way to receive random, trend and seasonality components since the variance is not changing over time.

Since each hour's consumption will be predicted separately in 24 distinct ARIMA models, the forecasting period is 2 for 2 days this time. Trend and seasonality components will be added after the random component after forecasting with each model. Then forecasted values of electricity consumption are received for each hour. 



```{r FORECASTING WITH ARIMA1, message=FALSE,warning=FALSE,eval=FALSE,class.source="fold-show"}
# empty vector for predictions of 2-days ahead 
desired_length <- 24 

current_date="2021-01-05" #This date is to be changed every day during the competition to include all the information.

# Forecasting for each hour.
ForecastedProd2<-c()

for (i in 0:23){
hour=i
combined_data<-rbind(df1, df_2,use.names=FALSE)
#Data for model fitting
combined_data<-combined_data[Date<=current_date]
#Hour filtering
combined_data=combined_data[combined_data$Hour==hour,c("Date","Consumption")]

#outlier removal
combined_data$clean_data=tsclean(combined_data$Consumption)
tsdisplay(combined_data$clean_data)

combined_data_decomp<-decompose(ts(combined_data$clean_data,freq=7),type="additive")

tsdisplay(combined_data_decomp$random)

noise<-combined_data_decomp$random
trend<-combined_data_decomp$trend
seasonality<-combined_data_decomp$seasonal

nahead=2 # 2 days later prediction

fitted=auto.arima(noise,seasonal = T,trace=F)
tsdisplay(residuals(fitted))

#Forecasting
forecasted1=forecast(fitted,h=nahead)
forecasted1



predicted_noise <- as.numeric(forecast(fitted,h=nahead)$mean)
last_trend_value <-tail(combined_data_decomp$trend[!is.na(combined_data_decomp$trend)],1)
seasonality<-combined_data_decomp$seasonal[7:8]

ProdForecast<-predicted_noise+last_trend_value+seasonality
ForecastedProd2<-c(ForecastedProd2,ProdForecast[2])
}

```

When residuals are analyzed, we see that residuals have constant variance and zero mean. However, there seems to be autocorrelation from the ACF and PACF plots. So, it is hard to say all assumptions are satisfied. 


### Forecasting with Exponential Smoothing

Forecasting with ETS models is quite similar to the forecasting with ARIMA. Thus, detailed code is omitted in this section. Predictions have been made in a very similar manner. Instead of auto.arima(), `forecast()` function with a `model=NULL` and `h=2` parameter is used.

```{r forecasting with ets, eval=FALSE, message=FALSE,warning=FALSE,class.source="fold-show"}
# used forecast function
exp_smoothing=forecast(combined_data,model=NULL,h=nhead)
empty_vec[i+1]<-exp_smoothing$mean[2]


```

### Forecasting with 'ARIMA with Regressors'

External factors such as temperature during the day can be good predictor and improve the model. In this setting, `max_temp`(maximum temperature) is used as external predictor. One ceveat is that, one needs to predict the temperature values for the forecasted days to implement them into the model. However, in our project setting, those values were already given to us. Max_temp values of the forecast horizon(2 days ahead) is stored in the data.frame named `df_2forecast`. The model is fitted in a very similar manner with previous ARIMA model except now, `xreg` component is added. Again, `auto.arima()` is used in the fitting phase to choose the best model, that is, the lowest AIC/BIC.


```{r forecasting with ARIMAX, message=FALSE,warning=FALSE,eval=FALSE ,class.source="fold-show"}
# fitting data
combined_data<-rbind(df1, df_2,use.names=FALSE)
current_date="2021-01-05" #This date is to be changed every day during the competition to include all the information.
combined_data<-combined_data[Date<=current_date] 
combined_data$max_temp<-apply(combined_data[,4:10],1,max)



# finding max temp of forecasting horizon(2 days)

df_2forecast<-df_2[(df_2$event_date>= "2021-01-08" & df_2$event_date < "2021-01-09"),] # change the days

#df_2forecast<-df_2[841:888,]  # change here, include forecasted days only
df_2forecast$max_temp<-apply(df_2forecast[,4:10],1,max)

# prediction vector, initially empty
desired_length <- 24 
empty_vec <- rep(NA, desired_length)

# forecasting each hour sepreately
for (i in 0:23){
hour=i
#model fitting  
combined_data$clean_data=tsclean(combined_data$Consumption)

#creating regression matrix
combined_data<-combined_data[Hour==hour]

reg_matrix=cbind(data.matrix(combined_data[,c("max_temp")])) # can add more if any other regressors exist


combined_data$clean_data<-ts(combined_data$clean_data,freq=7)


fitted_arimax=auto.arima(combined_data$clean_data,xreg=reg_matrix,seasonal=T,trace=F)


#forecasting part
xreg_forecast=data.matrix(df_2forecast[event_hour==hour]$max_temp)

  
nahead=2
forecasted=forecast(fitted_arimax,xreg=xreg_forecast,  h=nahead)
empty_vec[i+1]<-forecasted$mean[2]


# re-initialize the combined data for the loop 
combined_data<-rbind(df1, df_2,use.names=FALSE)

combined_data<-combined_data[Date<=current_date] 
combined_data$max_temp<-apply(combined_data[,4:10],1,max)
}


```


## Regression Approach


In this approach, mainly regression approach is utilized. Data is seperated into hourly datasets to predict individual hours of the day interested. However since it is expected to predict the electricity consumtion of the 7th day of January and we only have consumption values up to 5th of January, the consumption level of 6th of January is predicted and then used in the model to train the data. As for variables, additional electricity consumption data from EPIAS is used which is very highly correlated with electricity consumption data. Aditionally, those seven temperature variables are combined with PCA to create a seperate independent variable. When the plot of pca component and consumption is examined, it has been realized that consumption has a linear decreasing behavior up to 1.6 of component and exponential increase after that point so temp large and temp small variables are created. After pacf and acf plots are examined, we came up with 3 models. First one consists of variables mentioned above and additionally lag 1 consumption values. Second one has an additional lag 1 of residuals and the third one has the lag 1 of residuals without lag 1 of consumption. First model has been chosen as a result of summary(fit) and residual examination.


```{r reading data , message=FALSE,warning=FALSE,class.source="fold-hide"}
my_data <- fread("bulk_consumption_with_temp.csv")
my_data <- my_data[,Date:=as.Date(Date)]
my_data <- my_data[,woy:=weekdays(Date)]
my_data <- my_data[,Month:=as.factor(month(Date))]

###########
production_data <- fread('GercekZamanliUretim-01012010-01022021.csv')[,c('Tarih','Saat', 'Toplam (MWh)')]
```



```{r data manuplation, message=FALSE,warning=FALSE,class.source="fold-hide"}
setnames(production_data, 'Toplam (MWh)', "Total_Prod")

production_data[,Tarih := as.Date(Tarih, format = '%d.%m.%Y')]

production_data[,timestampt := ymd_hm(paste(Tarih, Saat))]

production_data[,hour := hour(timestampt)]

production_data[,Total_Prod:=as.numeric(gsub(",", ".", gsub("\\.", "",Total_Prod )))]

#Change date here read again if needed
production = production_data[ Tarih >= ymd(20170101)
                              & Tarih < ymd(20210108)
                              , .(Total_Prod,Tarih, hour)]

##########
my_data[,production:= production[,c('Total_Prod')]]



test_data = my_data[Date  == ymd(20210106) | Date  == ymd(20210107)]
my_data = my_data[Date  < ymd(20210106)]


str(my_data)

hour_0 = my_data[Hour == 0]
hour_1 = my_data[Hour == 1]
hour_2 = my_data[Hour == 2]
hour_3 = my_data[Hour == 3]
hour_4 = my_data[Hour == 4]
hour_5 = my_data[Hour == 5]
hour_6 = my_data[Hour == 6]
hour_7 = my_data[Hour == 7]
hour_8 = my_data[Hour == 8]
hour_9 = my_data[Hour == 9]
hour_10 = my_data[Hour == 10]
hour_11 = my_data[Hour == 11]
hour_12 = my_data[Hour == 12]
hour_13 = my_data[Hour == 13]
hour_14 = my_data[Hour == 14]
hour_15 = my_data[Hour == 15]
hour_16 = my_data[Hour == 16]
hour_17 = my_data[Hour == 17]
hour_18 = my_data[Hour == 18]
hour_19 = my_data[Hour == 19]
hour_20 = my_data[Hour == 20]
hour_21 = my_data[Hour == 21]
hour_22 = my_data[Hour == 22]
hour_23 = my_data[Hour == 23]

#str(hour_0)

```

```{r pca, message=FALSE,warning=FALSE,class.source="fold-hide"}
#no scaling, centering all are temp
hour_0.pca = prcomp(hour_0[,c(4:10)])
hour_0[,pca_1stcomp := hour_0.pca$x[,1]]

hour_1.pca = prcomp(hour_1[,c(4:10)])
hour_1[,pca_1stcomp := hour_1.pca$x[,1]]

hour_2.pca = prcomp(hour_2[,c(4:10)])
hour_2[,pca_1stcomp := hour_2.pca$x[,1]]

hour_3.pca = prcomp(hour_3[,c(4:10)])
hour_3[,pca_1stcomp := hour_3.pca$x[,1]]

hour_4.pca = prcomp(hour_4[,c(4:10)])
hour_4[,pca_1stcomp := hour_4.pca$x[,1]]

hour_5.pca = prcomp(hour_5[,c(4:10)])
hour_5[,pca_1stcomp := hour_5.pca$x[,1]]

hour_6.pca = prcomp(hour_6[,c(4:10)])
hour_6[,pca_1stcomp := hour_6.pca$x[,1]]

hour_7.pca = prcomp(hour_7[,c(4:10)])
hour_7[,pca_1stcomp := hour_7.pca$x[,1]]

hour_8.pca = prcomp(hour_8[,c(4:10)])
hour_8[,pca_1stcomp := hour_8.pca$x[,1]]

hour_9.pca = prcomp(hour_9[,c(4:10)])
hour_9[,pca_1stcomp := hour_9.pca$x[,1]]

hour_10.pca = prcomp(hour_10[,c(4:10)])
hour_10[,pca_1stcomp := hour_10.pca$x[,1]]

hour_11.pca = prcomp(hour_11[,c(4:10)])
hour_11[,pca_1stcomp := hour_11.pca$x[,1]]

hour_12.pca = prcomp(hour_12[,c(4:10)])
hour_12[,pca_1stcomp := hour_12.pca$x[,1]]

hour_13.pca = prcomp(hour_13[,c(4:10)])
hour_13[,pca_1stcomp := hour_13.pca$x[,1]]

hour_14.pca = prcomp(hour_14[,c(4:10)])
hour_14[,pca_1stcomp := hour_14.pca$x[,1]]

hour_15.pca = prcomp(hour_15[,c(4:10)])
hour_15[,pca_1stcomp := hour_15.pca$x[,1]]

hour_16.pca = prcomp(hour_16[,c(4:10)])
hour_16[,pca_1stcomp := hour_16.pca$x[,1]]

hour_17.pca = prcomp(hour_17[,c(4:10)])
hour_17[,pca_1stcomp := hour_17.pca$x[,1]]

hour_18.pca = prcomp(hour_18[,c(4:10)])
hour_18[,pca_1stcomp := hour_18.pca$x[,1]]

hour_19.pca = prcomp(hour_19[,c(4:10)])
hour_19[,pca_1stcomp := hour_19.pca$x[,1]]

hour_20.pca = prcomp(hour_20[,c(4:10)])
hour_20[,pca_1stcomp := hour_20.pca$x[,1]]

hour_21.pca = prcomp(hour_21[,c(4:10)])
hour_21[,pca_1stcomp := hour_21.pca$x[,1]]

hour_22.pca = prcomp(hour_22[,c(4:10)])
hour_22[,pca_1stcomp := hour_22.pca$x[,1]]

hour_23.pca = prcomp(hour_23[,c(4:10)])
hour_23[,pca_1stcomp := hour_23.pca$x[,1]]

summary(hour_23.pca)
#if you check the summary of pca, most of the variation is always explained by the first component

plot(zoo(hour_23[,list(Consumption, pca_1stcomp)],hour_23$Date))

ccf(hour_23$Consumption,hour_23$pca_1stcomp)
plot(hour_20[,list(pca_1stcomp,Consumption)])

plot(hour_0[,list(pca_1stcomp,Consumption)])
#up to the first pca 1.5 we see a liner derease and than a quadratic increase

```

```{r creating new variables based on temperature, message=FALSE,warning=FALSE,class.source="fold-hide"}
hour_0[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_0[,temp_small:=0]
hour_0[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_0[,temp_large:=0]
hour_0[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_1[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_1[,temp_small:=0]
hour_1[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_1[,temp_large:=0]
hour_1[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_2[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_2[,temp_small:=0]
hour_2[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_2[,temp_large:=0]
hour_2[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_3[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_3[,temp_small:=0]
hour_3[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_3[,temp_large:=0]
hour_3[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_4[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_4[,temp_small:=0]
hour_4[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_4[,temp_large:=0]
hour_4[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_5[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_5[,temp_small:=0]
hour_5[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_5[,temp_large:=0]
hour_5[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_6[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_6[,temp_small:=0]
hour_6[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_6[,temp_large:=0]
hour_6[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_7[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_7[,temp_small:=0]
hour_7[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_7[,temp_large:=0]
hour_7[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_8[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_8[,temp_small:=0]
hour_8[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_8[,temp_large:=0]
hour_8[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_9[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_9[,temp_small:=0]
hour_9[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_9[,temp_large:=0]
hour_9[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_10[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_10[,temp_small:=0]
hour_10[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_10[,temp_large:=0]
hour_10[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_11[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_11[,temp_small:=0]
hour_11[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_11[,temp_large:=0]
hour_11[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_12[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_12[,temp_small:=0]
hour_12[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_12[,temp_large:=0]
hour_12[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_13[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_13[,temp_small:=0]
hour_13[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_13[,temp_large:=0]
hour_13[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_14[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_14[,temp_small:=0]
hour_14[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_14[,temp_large:=0]
hour_14[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_15[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_15[,temp_small:=0]
hour_15[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_15[,temp_large:=0]
hour_15[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_16[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_16[,temp_small:=0]
hour_16[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_16[,temp_large:=0]
hour_16[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_17[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_17[,temp_small:=0]
hour_17[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_17[,temp_large:=0]
hour_17[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_18[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_18[,temp_small:=0]
hour_18[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_18[,temp_large:=0]
hour_18[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_19[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_19[,temp_small:=0]
hour_19[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_19[,temp_large:=0]
hour_19[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_20[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_20[,temp_small:=0]
hour_20[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_20[,temp_large:=0]
hour_20[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_21[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_21[,temp_small:=0]
hour_21[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_21[,temp_large:=0]
hour_21[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_22[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_22[,temp_small:=0]
hour_22[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_22[,temp_large:=0]
hour_22[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
##
hour_23[,abs_temp:=abs(pca_1stcomp-1.6)]
hour_23[,temp_small:=0]
hour_23[pca_1stcomp<1.6,temp_small:=abs_temp]

hour_23[,temp_large:=0]
hour_23[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
```

```{r pacf, message=FALSE,warning=FALSE}
pacf(hour_23$Consumption)

```

```{r creating lags, message=FALSE,warning=FALSE,class.source="fold-hide"}
hour_0[,lag_1 := shift(Consumption,1)]
hour_1[,lag_1 := shift(Consumption,1)]
hour_2[,lag_1 := shift(Consumption,1)]
hour_3[,lag_1 := shift(Consumption,1)]
hour_4[,lag_1 := shift(Consumption,1)]
hour_5[,lag_1 := shift(Consumption,1)]
hour_6[,lag_1 := shift(Consumption,1)]
hour_7[,lag_1 := shift(Consumption,1)]
hour_8[,lag_1 := shift(Consumption,1)]
hour_9[,lag_1 := shift(Consumption,1)]
hour_10[,lag_1 := shift(Consumption,1)]
hour_11[,lag_1 := shift(Consumption,1)]
hour_12[,lag_1 := shift(Consumption,1)]
hour_13[,lag_1 := shift(Consumption,1)]
hour_14[,lag_1 := shift(Consumption,1)]
hour_15[,lag_1 := shift(Consumption,1)]
hour_16[,lag_1 := shift(Consumption,1)]
hour_17[,lag_1 := shift(Consumption,1)]
hour_18[,lag_1 := shift(Consumption,1)]
hour_19[,lag_1 := shift(Consumption,1)]
hour_20[,lag_1 := shift(Consumption,1)]
hour_21[,lag_1 := shift(Consumption,1)]
hour_22[,lag_1 := shift(Consumption,1)]
hour_23[,lag_1 := shift(Consumption,1)]

```

```{r test manuplation, message=FALSE,warning=FALSE,class.source="fold-hide"}
transformer <- function(ind, loadings){
  r <- loadings * ind
  apply(r,2,sum)
}

test_data[,pca_1stcomp := t(apply(as.matrix(test_data[,c(4:10)]), 1, transformer, hour_23.pca$rotation))[,1]]


test_data[,abs_temp:=abs(pca_1stcomp-1.6)]
test_data[,temp_small:=0]
test_data[pca_1stcomp<1.6,temp_small:=abs_temp]

test_data[,temp_large:=0]
test_data[pca_1stcomp>=1.6,temp_large:=abs_temp^2]
head(test_data,10)

test_data[,lag_1:= c(my_data[Date == ymd(20210105), Consumption],
                     test_data[Date == ymd(20210106), Consumption])]
```

```{r fit 1 ,message=FALSE,warning=FALSE,class.source="fold-show"}
lm_fit1 = lm(Consumption ~ temp_small + temp_large + production + lag_1 + woy ,hour_23)

summary(lm_fit1)

plot(hour_23[,.(Consumption, residuals= residuals(lm_fit1))])

pacf(residuals(lm_fit1))

```

```{r fit 2,message=FALSE,warning=FALSE,class.source="fold-show"}
hour_23[,residual_lag1 := c(NA,lag(residuals(lm_fit1), 1))]

lm_fit2 = lm(Consumption ~ temp_small + temp_large + production + lag_1 + residual_lag1 + woy,hour_23)

summary(lm_fit2)
plot(hour_23[,.(Consumption, residuals= residuals(lm_fit2))])
#It seems like we are overfitting at this point, we can compare different models

```


```{r fit 3, message=FALSE, warning=FALSE,class.source="fold-show"}
lm_fit3 = lm(Consumption ~ temp_small + temp_large + production + residual_lag1 + woy,hour_23)
summary(lm_fit3)
plot(hour_23[,.(Consumption, residuals= residuals(lm_fit3))])

# It is determined that fit one can peform better on the test data, due to the overfitting in fit2 and shape of consumtion and residual plot in fit 3

hour_23[,residual_lag1:= NULL]


```

```{r mid-day prediction for 6th, message=FALSE,warning=FALSE,class.source="fold-show"}
mid_day_consumption = data.frame(n1 = as.numeric())

dt_list = list(hour_0, hour_1,hour_2,hour_3,hour_4,hour_5,hour_6,
               hour_7,hour_8,hour_9,hour_10,hour_11,hour_12,
               hour_13,hour_14,hour_15,hour_16,hour_17,hour_18,
               hour_19,hour_20,hour_21,hour_22,hour_23)

for (dt in dt_list){
  lm_fit = lm(Consumption ~ temp_small + temp_large + production + lag_1 + woy , dt)
  mid_day_consumption = rbind(mid_day_consumption, as.numeric(predict(lm_fit,test_data[Date == ymd(20210106) & Hour == unique(dt$Hour)])))
}
head(mid_day_consumption,10)
# We are using these consumption values to predict the next day

test_data[Date == ymd(20210106), Consumption := mid_day_consumption[1]]

```



```{r mid-day consumption manuplation,warning=FALSE,message=FALSE,class.source="fold-hide"}
hour_0 = rbind(hour_0,test_data[Date == ymd(20210106) & Hour == 0])
hour_1 = rbind(hour_1,test_data[Date == ymd(20210106) & Hour == 1])
hour_2 = rbind(hour_2,test_data[Date == ymd(20210106) & Hour == 2])
hour_3 = rbind(hour_3,test_data[Date == ymd(20210106) & Hour == 3])
hour_4 = rbind(hour_4,test_data[Date == ymd(20210106) & Hour == 4])
hour_5 = rbind(hour_5,test_data[Date == ymd(20210106) & Hour == 5])
hour_6 = rbind(hour_5,test_data[Date == ymd(20210106) & Hour == 6])
hour_7 = rbind(hour_6,test_data[Date == ymd(20210106) & Hour == 7])
hour_8 = rbind(hour_7,test_data[Date == ymd(20210106) & Hour == 8])
hour_9 = rbind(hour_8,test_data[Date == ymd(20210106) & Hour == 9])
hour_10 = rbind(hour_10,test_data[Date == ymd(20210106) & Hour == 10])
hour_11 = rbind(hour_11,test_data[Date == ymd(20210106) & Hour == 11])
hour_12 = rbind(hour_12,test_data[Date == ymd(20210106) & Hour == 12])
hour_13 = rbind(hour_13,test_data[Date == ymd(20210106) & Hour == 13])
hour_14 = rbind(hour_14,test_data[Date == ymd(20210106) & Hour == 14])
hour_15 = rbind(hour_15,test_data[Date == ymd(20210106) & Hour == 15])
hour_16 = rbind(hour_16,test_data[Date == ymd(20210106) & Hour == 16])
hour_17 = rbind(hour_17,test_data[Date == ymd(20210106) & Hour == 17])
hour_18 = rbind(hour_18,test_data[Date == ymd(20210106) & Hour == 18])
hour_19 = rbind(hour_19,test_data[Date == ymd(20210106) & Hour == 19])
hour_20 = rbind(hour_20,test_data[Date == ymd(20210106) & Hour == 20])
hour_21 = rbind(hour_21,test_data[Date == ymd(20210106) & Hour == 21])
hour_22 = rbind(hour_22,test_data[Date == ymd(20210106) & Hour == 22])
hour_23 = rbind(hour_23,test_data[Date == ymd(20210106) & Hour == 23])

```


```{r prediction for 7th, message=FALSE,warning=FALSE,class.source="fold-show"}
predictions = data.frame(n1 = as.numeric())

for (dt in dt_list){
  lm_fit = lm(Consumption ~ temp_small + temp_large + production + lag_1 + woy , dt)
  predictions = rbind(predictions, as.numeric(predict(lm_fit,test_data[Date == ymd(20210107) & Hour == unique(dt$Hour)])))
}

```


# Results

To decide on which model performs best, we need to evaluate its accuracy based on some metric. In our case, `WMAPE` is used to select the best model. For that, train data is selected from the `Date=2017-01-01` to `Date=2021-01-05`. And test data is selected as  `Date=2021-01-07` consumption data. 

As for error metric wmape is used which a variant of MAPE in which errors are weighted by values of actuals. Therefore, it is not affected by low and high actual values as much as MAPE does. 

```{r error test function,class.source="fold-show", message=FALSE,warning=FALSE}
error_test <- function(actual, forecasted){
  n=length(actual)
  error = actual-forecasted
  mean=mean(actual)
  sd=sd(actual)
  bias = sum(error)/sum(actual)
  mape = sum(abs(error/actual))/n
  mad = sum(abs(error))/n
  wmape = mad/mean
  MPE = sum(error/actual)/n
  df = data.frame(n,mean,sd,bias,mape,mad,wmape,MPE)
  return(df)
}


```

## Time Series Analysis Approach

### ARIMA 


```{r train-test data1,message=FALSE,warning=FALSE,class.source="fold-hide"}
df1 = df1[order(Date,Hour)]
train_data=df1[Date<="2021-01-05"]
test_data=df1[Date=="2021-01-07"]
#ARIMA MODEL ACCURACY FOR THE DATE 2021-01-07
# empty vector for predictions of 2-days ahead 
desired_length <- 24 
empty_vec <- rep(NA, desired_length)

current_date="2021-01-05" #This date is to be changed every day during the competition to include all the information.
#
initial_df=df1
# Forecasting for each hour.
for (i in 0:23){
hour=i
df1=initial_df
#combined_data<-rbind(df1, df_2,use.names=FALSE)
#Data for model fitting
df1<-df1[Date<=current_date]
#Hour filtering
df1=df1[df1$Hour==hour,c("Date","Consumption")]

#outlier removal
df1$clean_data=tsclean(df1$Consumption)

df1<-ts(df1$clean_data,freq=7)


nahead=2 # 2 days later prediction

fitted=auto.arima(df1,seasonal = T,trace=F)  

#Forecasting
forecasted1=forecast(fitted,h=nahead)
forecasted1

empty_vec[i+1]<-forecasted1$mean[2]
}



```


```{r accuracy, message=FALSE,warning=FALSE,class.source="fold-hide"}
error_test(test_data$Consumption,empty_vec)

```

### ARIMA on Random Component of the Time Series Object

```{r train-test data,message=FALSE,warning=FALSE,class.source="fold-hide"}
#ARIMA MODEL ACCURACY FOR THE DATE 2021-01-07
# empty vector for predictions of 2-days ahead 
desired_length <- 24 
ForecastedProd2<-c()

current_date="2021-01-05" #This date is to be changed every day during the competition to include all the information.

# Forecasting for each hour.
for (i in 0:23){
hour=i
df1=initial_df
#combined_data<-rbind(df1, df_2,use.names=FALSE)
#Data for model fitting
df1<-df1[Date<=current_date]
#Hour filtering
df1=df1[df1$Hour==hour,c("Date","Consumption")]

#outlier removal
df1$clean_data=tsclean(df1$Consumption)


combined_data_decomp<-decompose(ts(df1$clean_data,freq=7),type="additive")

#tsdisplay(combined_data_decomp$random)

noise<-combined_data_decomp$random
trend<-combined_data_decomp$trend
seasonality<-combined_data_decomp$seasonal

nahead=2 # 2 days later prediction

fitted=auto.arima(noise,seasonal = T,trace=F)


predicted_noise <- as.numeric(forecast(fitted,h=nahead)$mean)
last_trend_value <-tail(combined_data_decomp$trend[!is.na(combined_data_decomp$trend)],1)
seasonality<-combined_data_decomp$seasonal[7:8]

ProdForecast<-predicted_noise+last_trend_value+seasonality
ForecastedProd2<-c(ForecastedProd2,ProdForecast[2])

}



```


```{r message=FALSE,warning=FALSE,class.source="fold-hide"}
error_test(test_data$Consumption,ForecastedProd2)

```

### ETS

```{r ets train, message=FALSE,warning=FALSE,class.source="fold-hide"}


df1 = fread('bulk_consumption_with_temp.csv', header=TRUE)
df1[,Date:=as.Date(Date,"%m/%d/%Y")]
df1 = df1[order(Date,Hour)]
desired_length <- 24 
empty_vec <- rep(NA, desired_length)

current_date="2021-01-05"


initial_df=df1

for (i in 0:23){
hour=i

df1=initial_df

df1<-df1[Date<=current_date] # change here 
df1=df1[df1$Hour==hour,c("Date","Consumption")]
#outlier removal
df1$clean_data=tsclean(df1$Consumption)

df1<-ts(df1$clean_data,freq=7)


nhead=2 # 2 days later prediction

exp_smoothing3=forecast(df1,model=NULL,h=nhead)
empty_vec[i+1]<-exp_smoothing3$mean[2]
}
```

```{r accuracy1, message=FALSE,warning=FALSE}
error_test(test_data$Consumption,empty_vec)

```

### ARIMAX 


```{r message=FALSE,warning=FALSE,class.source="fold-hide"}
df1 = fread('bulk_consumption_with_temp.csv', header=TRUE)
df1[,Date:=as.Date(Date,"%m/%d/%Y")]
df1 = df1[order(Date,Hour)]
desired_length <- 24 
empty_vec <- rep(NA, desired_length)

current_date="2021-01-05"


initial_df=df1

df1$max_temp<-apply(df1[,4:10],1,max)

```

```{r forecasting , message=FALSE,warning=FALSE}
# finding max temp of forecasted day
df1forecast<-df1[(df1$Date<= "2021-01-07" & df1$Date >= "2021-01-06"),]
  # change here, include forecasted days only
df1forecast$max_temp<-apply(df1forecast[,4:10],1,max)


desired_length <- 24 
empty_vec <- rep(NA, desired_length)

for (i in 0:23){
hour=i
#model fitting  
df1$clean_data=tsclean(df1$Consumption)

#creating regression matrix
df1<-df1[Hour==hour]

reg_matrix=cbind(data.matrix(df1[,c("max_temp")])) # can add more if any other regressors exist


df1$clean_data<-ts(df1$clean_data,freq=7)


fitted_arimax=auto.arima(df1$clean_data,xreg=reg_matrix,seasonal=T,trace=F)




#forecasting part
xreg_forecast=data.matrix(df1forecast[Hour==hour]$max_temp)

  
nahead=2
forecasted=forecast(fitted_arimax,xreg=xreg_forecast,  h=nahead)
empty_vec[i+1]<-forecasted$mean[2]

df1=initial_df

df1<-df1[Date<=current_date]# change here
df1$max_temp<-apply(df1[,4:10],1,max)
}


  




```

```{r message=FALSE,warning=FALSE,class.source="fold-hide"}
error_test(test_data$Consumption,empty_vec)

```



## Regression Approach

```{r, message=FALSE,warning=FALSE,class.source="fold-hide"}

error_test(test_data[Date == ymd(20210107)]$Consumption, predictions[1] )

```

Since we chose wmape as the main performance metric for the reason we explained earlier, the model with the lowest wmape value will be chosen as the best forecasting model. In other words, as a result of our recent studies, exponential smoothing and then regression were the two models that gave the best results. 

However, since we focus only on arima models during submission period, we have chosen our ARIMAX model, which gives the best wmape value among the arima models. 

# Conclusions and Future Work 


For the project, first, we have analyzed the given data, did data manipulation and then developed several forecasting models related to what we have learned throughoout the course. The models have been tested statistically and best model is selected. However, during the competition, we mostly used ARIMA models excluding regression approach. Although ARIMAX( arima with regressors) was also used for some of the days, we could have improved the model by adding some other predictors such as min temp, humidity, gas consumption. Surprisingly ETS model gave the best result( lowest WMAPE value) during the testing phase. Thus, using that approach more during the competition might be better for us in terms of our ranking. 

Also, we know that our study and the results of the forecast would be very beneficial for the market participants as forecasting the next-day electricity consumption and price could be used by participants of the da-ahead market.


# Code

Codes in the RMD file [here](https://bu-ie-360.github.io/fall20-fatmadumlupinar/files/project/IE360_Project_Report.html)
